{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Clasificación de Plantas con MaxViT\n",
                "\n",
                "Este notebook clasifica imágenes en 3 clases: **manzana**, **patatas**, **rosas**.\n",
                "\n",
                "Diseñado para ser ejecutado en Google Colab."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 1: Instalación de dependencias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision timm pillow matplotlib numpy --quiet"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 2: Imports y configuración del dispositivo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import timm\n",
                "\n",
                "# Configuración del dispositivo\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Usando dispositivo: {device}\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"timm version: {timm.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 3: Montar Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "print(\"Google Drive montado correctamente.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 4: Configuración del dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ruta del dataset en Google Drive\n",
                "DRIVE_PATH = '/content/drive/MyDrive/data_balanced'\n",
                "\n",
                "# Verificar que la ruta existe\n",
                "if os.path.exists(DRIVE_PATH):\n",
                "    print(f\"✓ Ruta encontrada: {DRIVE_PATH}\")\n",
                "    print(f\"  Contenido: {os.listdir(DRIVE_PATH)}\")\n",
                "else:\n",
                "    print(f\"✗ ERROR: No se encuentra la ruta {DRIVE_PATH}\")\n",
                "    print(\"  Verifica que la carpeta existe en Google Drive\")\n",
                "\n",
                "# Clases del dataset\n",
                "CLASSES = ['manzana', 'patatas', 'rosas']\n",
                "NUM_CLASSES = len(CLASSES)\n",
                "\n",
                "# Proporciones de división\n",
                "TRAIN_RATIO = 0.70\n",
                "VAL_RATIO = 0.15\n",
                "TEST_RATIO = 0.15\n",
                "\n",
                "print(f\"Ruta del dataset: {DRIVE_PATH}\")\n",
                "print(f\"Número de clases: {NUM_CLASSES}\")\n",
                "print(f\"Clases: {CLASSES}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 5: Definición del Dataset personalizado"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PlantDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset personalizado para cargar imágenes de plantas.\n",
                "    Estructura esperada:\n",
                "        root/\n",
                "            manzana/\n",
                "                img1.jpg\n",
                "                img2.jpg\n",
                "            patatas/\n",
                "                img1.jpg\n",
                "                ...\n",
                "            rosas/\n",
                "                img1.jpg\n",
                "                ...\n",
                "    \"\"\"\n",
                "    def __init__(self, root_dir, classes, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.classes = classes\n",
                "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
                "        self.transform = transform\n",
                "        self.samples = []\n",
                "        \n",
                "        # Cargar todas las imágenes de cada clase\n",
                "        for class_name in classes:\n",
                "            class_path = os.path.join(root_dir, class_name)\n",
                "            if os.path.exists(class_path):\n",
                "                for img_name in os.listdir(class_path):\n",
                "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
                "                        img_path = os.path.join(class_path, img_name)\n",
                "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
                "            else:\n",
                "                print(f\"Advertencia: No existe la carpeta {class_path}\")\n",
                "        \n",
                "        print(f\"Total de imágenes cargadas: {len(self.samples)}\")\n",
                "        for cls in classes:\n",
                "            count = sum(1 for s in self.samples if s[1] == self.class_to_idx[cls])\n",
                "            print(f\"  - {cls}: {count} imágenes\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_path, label = self.samples[idx]\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        return image, label"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 6: Definición de transformaciones"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transformaciones para entrenamiento (con aumento de datos)\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize((256, 256)),\n",
                "    transforms.RandomCrop(224),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomVerticalFlip(),\n",
                "    transforms.RandomRotation(30),\n",
                "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
                "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# Transformaciones para validación/test (sin aumento)\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "print(\"Train transform:\", train_transform)\n",
                "print(\"\\nVal transform:\", val_transform)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 7: Clase auxiliar para subset con transformaciones"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SubsetWithTransform(Dataset):\n",
                "    \"\"\"Wrapper para aplicar transformaciones a un subset.\"\"\"\n",
                "    def __init__(self, subset, original_dataset, transform):\n",
                "        self.indices = list(subset)\n",
                "        self.original_dataset = original_dataset\n",
                "        self.transform = transform\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.indices)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        original_idx = self.indices[idx]\n",
                "        img_path, label = self.original_dataset.samples[original_idx]\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        return image, label"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 8: Carga y división del dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cargar dataset completo (sin transformaciones para dividir)\n",
                "full_dataset = PlantDataset(\n",
                "    root_dir=DRIVE_PATH,\n",
                "    classes=CLASSES,\n",
                "    transform=None  # Aplicaremos transformaciones después de dividir\n",
                ")\n",
                "\n",
                "# Calcular tamaños para la división\n",
                "total_size = len(full_dataset)\n",
                "train_size = int(TRAIN_RATIO * total_size)\n",
                "val_size = int(VAL_RATIO * total_size)\n",
                "test_size = total_size - train_size - val_size\n",
                "\n",
                "print(f\"\\nDivisión del dataset:\")\n",
                "print(f\"  - Train: {train_size} ({TRAIN_RATIO*100:.0f}%)\")\n",
                "print(f\"  - Val: {val_size} ({VAL_RATIO*100:.0f}%)\")\n",
                "print(f\"  - Test: {test_size} ({TEST_RATIO*100:.0f}%)\")\n",
                "\n",
                "# Dividir dataset con semilla fija para reproducibilidad\n",
                "generator = torch.Generator().manual_seed(42)\n",
                "train_indices, val_indices, test_indices = random_split(\n",
                "    range(total_size), \n",
                "    [train_size, val_size, test_size],\n",
                "    generator=generator\n",
                ")\n",
                "\n",
                "# Crear datasets con transformaciones apropiadas\n",
                "train_dataset = SubsetWithTransform(train_indices, full_dataset, train_transform)\n",
                "val_dataset = SubsetWithTransform(val_indices, full_dataset, val_transform)\n",
                "test_dataset = SubsetWithTransform(test_indices, full_dataset, val_transform)\n",
                "\n",
                "print(f\"\\nDatasets creados:\")\n",
                "print(f\"  - Train: {len(train_dataset)} muestras\")\n",
                "print(f\"  - Val: {len(val_dataset)} muestras\")\n",
                "print(f\"  - Test: {len(test_dataset)} muestras\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 9: Creación de DataLoaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 32\n",
                "\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=True,\n",
                "    num_workers=2\n",
                ")\n",
                "\n",
                "val_loader = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=2\n",
                ")\n",
                "\n",
                "test_loader = DataLoader(\n",
                "    test_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=2\n",
                ")\n",
                "\n",
                "print(f\"\\nDataLoaders creados:\")\n",
                "print(f\"  - Train batches: {len(train_loader)}\")\n",
                "print(f\"  - Val batches: {len(val_loader)}\")\n",
                "print(f\"  - Test batches: {len(test_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 10: Función para crear modelo de fine-tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_finetune_model(num_classes, model_name='maxvit_tiny_tf_224.in1k', freeze_backbone=True):\n",
                "    \"\"\"\n",
                "    Crea un modelo MaxViT para fine-tuning.\n",
                "    \n",
                "    Args:\n",
                "        num_classes: Número de clases para clasificación\n",
                "        model_name: Nombre del modelo MaxViT preentrenado\n",
                "        freeze_backbone: Si True, congela el backbone y solo entrena la cabeza\n",
                "    \n",
                "    Returns:\n",
                "        Modelo configurado para fine-tuning\n",
                "    \"\"\"\n",
                "    model = timm.create_model(\n",
                "        model_name,\n",
                "        pretrained=True,\n",
                "        num_classes=num_classes\n",
                "    )\n",
                "    \n",
                "    if freeze_backbone:\n",
                "        # Congelar todo excepto la cabeza\n",
                "        for name, param in model.named_parameters():\n",
                "            if 'head' not in name:\n",
                "                param.requires_grad = False\n",
                "    \n",
                "    # Contar parámetros\n",
                "    total_params = sum(p.numel() for p in model.parameters())\n",
                "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "    \n",
                "    print(f\"Modelo: {model_name}\")\n",
                "    print(f\"Clases: {num_classes}\")\n",
                "    print(f\"Parámetros totales: {total_params:,}\")\n",
                "    print(f\"Parámetros entrenables: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 11: Funciones de entrenamiento y evaluación"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
                "    \"\"\"\n",
                "    Entrena el modelo durante una época.\n",
                "    \"\"\"\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
                "        inputs, targets = inputs.to(device), targets.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, targets)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item()\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += targets.size(0)\n",
                "        correct += predicted.eq(targets).sum().item()\n",
                "        \n",
                "        if (batch_idx + 1) % 10 == 0:\n",
                "            print(f'Batch [{batch_idx+1}/{len(train_loader)}] '\n",
                "                  f'Loss: {running_loss/(batch_idx+1):.4f} '\n",
                "                  f'Acc: {100.*correct/total:.2f}%')\n",
                "    \n",
                "    return running_loss / len(train_loader), 100. * correct / total\n",
                "\n",
                "\n",
                "def evaluate(model, val_loader, criterion, device):\n",
                "    \"\"\"\n",
                "    Evalúa el modelo en el conjunto de validación.\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for inputs, targets in val_loader:\n",
                "            inputs, targets = inputs.to(device), targets.to(device)\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs, targets)\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            _, predicted = outputs.max(1)\n",
                "            total += targets.size(0)\n",
                "            correct += predicted.eq(targets).sum().item()\n",
                "    \n",
                "    return running_loss / len(val_loader), 100. * correct / total"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 12: Entrenamiento - Fase 1 (solo cabeza)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"FASE 1: Entrenando solo la cabeza del modelo\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "model = create_finetune_model(num_classes=NUM_CLASSES, freeze_backbone=True).to(device)\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)  # LR alto para la cabeza\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
                "\n",
                "EPOCHS_PHASE1 = 5\n",
                "for epoch in range(EPOCHS_PHASE1):\n",
                "    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS_PHASE1} ===\")\n",
                "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
                "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
                "    scheduler.step()\n",
                "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
                "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 13: Entrenamiento - Fase 2 (fine-tuning completo)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"FASE 2: Fine-tuning completo\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Descongelar todos los parámetros\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = True\n",
                "\n",
                "# Contar parámetros ahora\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # LR bajo para fine-tuning\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
                "\n",
                "EPOCHS_PHASE2 = 10\n",
                "best_val_acc = 0.0\n",
                "for epoch in range(EPOCHS_PHASE2):\n",
                "    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS_PHASE2} ===\")\n",
                "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
                "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
                "    scheduler.step()\n",
                "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
                "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
                "    \n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        print(f\"✓ Nuevo mejor modelo guardado (Val Acc: {val_acc:.2f}%)\")\n",
                "\n",
                "print(\"\\n✅ Entrenamiento completado.\")\n",
                "print(f\"Mejor accuracy de validación: {best_val_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 14: Evaluación final en conjunto de test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"EVALUACIÓN EN CONJUNTO DE TEST\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
                "print(f\"Test Loss: {test_loss:.4f}\")\n",
                "print(f\"Test Accuracy: {test_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 15: Matriz de confusión"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import seaborn as sns\n",
                "\n",
                "def get_all_predictions(model, dataloader, device):\n",
                "    \"\"\"Obtiene todas las predicciones para el dataloader.\"\"\"\n",
                "    model.eval()\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in dataloader:\n",
                "            inputs = inputs.to(device)\n",
                "            outputs = model(inputs)\n",
                "            _, preds = outputs.max(1)\n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.numpy())\n",
                "    \n",
                "    return np.array(all_preds), np.array(all_labels)\n",
                "\n",
                "# Obtener predicciones\n",
                "preds, labels = get_all_predictions(model, test_loader, device)\n",
                "\n",
                "# Matriz de confusión\n",
                "cm = confusion_matrix(labels, preds)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=CLASSES, yticklabels=CLASSES)\n",
                "plt.xlabel('Predicción')\n",
                "plt.ylabel('Real')\n",
                "plt.title('Matriz de Confusión')\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "# Reporte de clasificación\n",
                "print(\"\\nReporte de clasificación:\")\n",
                "print(classification_report(labels, preds, target_names=CLASSES))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bloque 16: Guardar modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_model(model, path, model_name, num_classes, classes, epoch=None):\n",
                "    \"\"\"\n",
                "    Guarda el modelo con metadatos.\n",
                "    \"\"\"\n",
                "    checkpoint = {\n",
                "        'model_name': model_name,\n",
                "        'num_classes': num_classes,\n",
                "        'classes': classes,\n",
                "        'state_dict': model.state_dict(),\n",
                "        'epoch': epoch\n",
                "    }\n",
                "    torch.save(checkpoint, path)\n",
                "    print(f\"Modelo guardado en: {path}\")\n",
                "\n",
                "\n",
                "def load_model(path, device='cuda'):\n",
                "    \"\"\"\n",
                "    Carga un modelo guardado.\n",
                "    \"\"\"\n",
                "    checkpoint = torch.load(path, map_location=device)\n",
                "    \n",
                "    model = timm.create_model(\n",
                "        checkpoint['model_name'],\n",
                "        pretrained=False,\n",
                "        num_classes=checkpoint['num_classes']\n",
                "    )\n",
                "    model.load_state_dict(checkpoint['state_dict'])\n",
                "    model = model.to(device)\n",
                "    model.eval()\n",
                "    \n",
                "    print(f\"Modelo cargado: {checkpoint['model_name']}\")\n",
                "    print(f\"Clases: {checkpoint.get('classes', 'N/A')}\")\n",
                "    print(f\"Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
                "    \n",
                "    return model\n",
                "\n",
                "\n",
                "# Guardar el modelo entrenado\n",
                "MODEL_PATH = 'maxvit_plant_classifier.pth'\n",
                "save_model(\n",
                "    model, \n",
                "    MODEL_PATH, \n",
                "    'maxvit_tiny_tf_224.in1k', \n",
                "    num_classes=NUM_CLASSES,\n",
                "    classes=CLASSES,\n",
                "    epoch=EPOCHS_PHASE1 + EPOCHS_PHASE2\n",
                ")\n",
                "\n",
                "print(\"\\n✅ Script completado exitosamente!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}