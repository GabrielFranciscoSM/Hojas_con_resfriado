\section{Methods}

\subsection{Modelos de Clasificación}

En nuestro problema tenemos dos etapas de clasificación de imágenes. 
Primero, una clasificación de hojas en tipos de plantas, y luego una 
clasificación de las enfermedades presentes en las hojas de cada tipo de planta.

Estos son dos problemas bastante diferentes en cuanto a la complejidad de la tarea,
principalmente porque en la primera etapa las diferencias entre clases son más notorias
(muy diferentes tipos de plantas), mientras que en la segunda etapa las diferencias
son más sutiles (diferentes enfermedades que pueden afectar a la misma planta,
y que a veces pueden parecerse mucho entre sí). Además, nos enfretamos al problema 
de la diferencia de tamaños de las imágenes de las hojas y la de sus enfermedades.

Con esto en mente, tiene sentido estudiar una amplia gama de modelos de clasificación,
y elegir los más adecuados para cada etapa del problema.

He dividido el estudio inicial de los modelos en dos partes: arquitecturas clásicas
y arquitecturas modernas basadas en Transformers. En las tablas \ref{tab:model_comparison}
y \ref{tab:modern_model_comparison} se presentan las comparaciones detalladas de
las diferentes arquitecturas estudiadas.

\begin{table*}[htbp]
    \centering
    \scriptsize % Texto más pequeño para ajustar el contenido
    \renewcommand{\arraystretch}{1.3} % Altura de filas ajustada
    \begin{tabularx}{\textwidth}{@{} >{\bfseries}p{2.2cm} X X X X @{}}
        \toprule
        Característica & \textbf{GoogLeNet (Inception)} & \textbf{ResNet} & \textbf{EfficientNet (V1)} & \textbf{EfficientNetV2} \\
        \midrule
        
        Primera Publicación & 
        2014 (Szegedy et al., Google) &
        2015 (He et al.) & 
        2019 (Tan \& Le, Google) & 
        2021 (Tan \& Le, Google) \\

        Idea Principal & 
        Módulos Inception con convoluciones paralelas de múltiples escalas &
        Conexiones residuales (skip) para permitir el entrenamiento de redes muy profundas & 
        Escalado compuesto: escalar conjuntamente profundidad, anchura y resolución usando un coeficiente fijo & 
        Escalado compuesto mejorado + entrenamiento más rápido: arquitectura más inteligente + regularización adaptativa \\

        Bloque Básico & 
        Módulo Inception: convoluciones paralelas 1$\times$1, 3$\times$3, 5$\times$5 + max pooling, concatenadas &
        $\bullet$ BasicBlock (2$\times$ 3$\times$3 convs) para ResNet18/34 \newline $\bullet$ Bottleneck (1$\times$1 $\to$ 3$\times$3 $\to$ 1$\times$1 convs) para ResNet50+ & 
        MBConv (Mobile Inverted Bottleneck): \newline 1$\times$1 exp $\to$ 3$\times$3 depthwise $\to$ SE $\to$ 1$\times$1 proj & 
        Fused-MBConv (etapas iniciales): conv 3$\times$3 regular en lugar de 1$\times$1+depthwise \newline MBConv (etapas posteriores) \newline SE solo en bloques posteriores \\

        Conexiones Skip & 
        \xmark\ No (pero versiones posteriores como Inception-ResNet sí) &
        \cmark\ Sí (residual aditivo) & 
        \cmark\ Sí (MBConv usa residual cuando strides=1) & 
        \cmark\ Sí \\

        Atención & 
        \xmark\ No &
        \xmark\ No & 
        \cmark\ Squeeze-and-Excitation (SE) en cada bloque & 
        \cmark\ SE, pero solo en etapas posteriores (para reducir overhead) \\

        Método de Escalado & 
        Manual: aumento de módulos Inception apilados &
        Aumento manual de profundidad \newline (18 $\to$ 34 $\to$ 50 $\to$ 101 $\to$ 152) & 
        Escalado compuesto uniforme: \newline Profundidad $\times \alpha$, Anchura $\times \beta$, Res $\times \gamma$ & 
        Escalado compuesto no uniforme + aprendizaje progresivo (imágenes pequeñas $\to$ grandes durante entrenamiento) \\

        Resolución de Entrada & 
        Fija (224$\times$224) &
        Fija (usualmente 224$\times$224), pero flexible & 
        Escala con el tamaño del modelo: \newline B0: 224 $\to$ B7: 600 & 
        Menor que V1 para la misma precisión: \newline S: 384, M: 480, L: 480–512 \\

        Tamaños Típicos & 
        $\bullet$ GoogLeNet (6.8M params) \newline $\bullet$ Inception-v3 (23.8M) \newline $\bullet$ Inception-v4 (42.7M) &
        $\bullet$ ResNet18 (11M params) \newline $\bullet$ ResNet34 (21M) \newline $\bullet$ ResNet50 (25M) \newline $\bullet$ ResNet101 (44M) & 
        $\bullet$ B0 (5.3M) \newline $\bullet$ B3 (12M) \newline $\bullet$ B5 (30M) \newline $\bullet$ B7 (66M) & 
        $\bullet$ S (21M) \newline $\bullet$ M (54M) \newline $\bullet$ L (120M) \\

        Precisión (ImageNet) & 
        $\bullet$ GoogLeNet: $\sim$69.8\% \newline $\bullet$ Inception-v3: $\sim$78.8\% &
        $\bullet$ ResNet50: $\sim$76\% \newline $\bullet$ ResNet101: $\sim$78\% & 
        $\bullet$ B0: $\sim$77\% \newline $\bullet$ B3: $\sim$82\% \newline $\bullet$ B7: $\sim$84.4\% & 
        $\bullet$ S: $\sim$83.9\% \newline $\bullet$ M: $\sim$85.2\% \newline $\bullet$ L: $\sim$85.7\% \\

        Velocidad de Entrenamiento & 
        Moderada &
        Moderada & 
        Lenta (debido a entradas de alta resolución y profundidad) & 
        Mucho más rápida (hasta 11$\times$ vs V1 con precisión similar) \\

        Eficiencia de Inferencia & 
        Buena; diseño eficiente en parámetros para su época &
        Buena para modelos pequeños (18/34); más pesada para 101+ & 
        Muy eficiente en parámetros, pero alta resolución afecta latencia & 
        Mejor relación FLOPs/precisión; optimizado para TPU/GPU \\

        Mejor Para & 
        $\bullet$ Comprensión histórica de CNNs \newline $\bullet$ Extracción de características multi-escala &
        $\bullet$ Modelos base \newline $\bullet$ Transfer learning con datos limitados \newline $\bullet$ Prototipado rápido & 
        $\bullet$ Despliegue con recursos limitados \newline $\bullet$ Cuando se necesita alta precisión con menos parámetros & 
        $\bullet$ Sistemas de producción que necesitan velocidad + precisión \newline $\bullet$ Entrenamiento desde cero o fine-tuning \\

        fastai / PyTorch & 
        \cmark\ Nativo en torchvision (\texttt{googlenet}, \texttt{inception\_v3}) &
        \cmark\ Nativo en torchvision y fastai & 
        \cmark\ Vía \texttt{timm} (ej., \texttt{efficientnet\_b0}) & 
        \cmark\ Vía \texttt{timm} (ej., \texttt{efficientnetv2\_s}) \\

        Debilidad Principal & 
        Arquitectura obsoleta; menor precisión que modelos modernos &
        Eficiencia de parámetros subóptima; obsoleto vs modelos modernos & 
        Entrenamiento lento; SE añade overhead computacional; alta resolución = mucha memoria & 
        Ecosistema ligeramente menos maduro; no está en torchvision \\

        \bottomrule
    \end{tabularx}
    \caption{Comparación de las Arquitecturas GoogLeNet (Inception), ResNet, EfficientNetV1 y EfficientNetV2}
    \label{tab:model_comparison}
\end{table*}

\begin{table*}[htbp]
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{@{} >{\bfseries}p{2.2cm} X X X @{}}
        \toprule
        Característica & \textbf{ConvNeXt} & \textbf{Swin Transformer} & \textbf{MaxViT} \\
        \midrule
        
        Primera Publicación & 
        2022 (Liu et al., Meta/FAIR) & 
        2021 (Liu et al., Microsoft) & 
        2022 (Tu et al., Google) \\

        Idea Principal & 
        Modernización de ResNet con técnicas inspiradas en Transformers, demostrando que CNNs puras pueden competir & 
        Transformer jerárquico con atención en ventanas locales y desplazamiento para conexión entre ventanas & 
        Combinación de convoluciones + atención local (block) + atención global diluida (grid) en cada bloque \\

        Tipo de Arquitectura & 
        CNN pura (sin atención) & 
        Transformer puro (sin convoluciones en backbone) & 
        Híbrido CNN + Transformer \\

        Bloque Básico & 
        Inverted bottleneck con depthwise conv 7$\times$7, LayerNorm, GELU & 
        Swin Transformer Block: \newline Window-MSA $\to$ MLP $\to$ Shifted-Window-MSA $\to$ MLP & 
        MBConv $\to$ Block Attention $\to$ Grid Attention \\

        Mecanismo de Atención & 
        \xmark\ No usa atención (solo convoluciones) & 
        \cmark\ Multi-head Self-Attention en ventanas 7$\times$7, con shift en capas alternas & 
        \cmark\ Block Attention (local en ventanas) + Grid Attention (global diluida) \\

        Receptive Field Global & 
        Solo en capas profundas (por apilamiento) & 
        Gradual mediante shifted windows & 
        \cmark\ Desde las primeras capas (via Grid Attention) \\

        Complejidad Computacional & 
        O(n) lineal respecto a resolución & 
        O(n) lineal (atención local en ventanas fijas) & 
        O(n) lineal (block + grid attention) \\

        Representación Jerárquica & 
        \cmark\ Sí (4 etapas con downsampling) & 
        \cmark\ Sí (patch merging entre etapas) & 
        \cmark\ Sí (estructura multi-escala) \\

        Resolución de Entrada & 
        224$\times$224 (escalable) & 
        224$\times$224 (escalable a alta resolución) & 
        224$\times$224 a 512$\times$512 \\

        Tamaños Típicos & 
        $\bullet$ ConvNeXt-T (29M) \newline $\bullet$ ConvNeXt-S (50M) \newline $\bullet$ ConvNeXt-B (89M) \newline $\bullet$ ConvNeXt-L (198M) & 
        $\bullet$ Swin-T (29M) \newline $\bullet$ Swin-S (50M) \newline $\bullet$ Swin-B (88M) \newline $\bullet$ Swin-L (197M) & 
        $\bullet$ MaxViT-T (31M) \newline $\bullet$ MaxViT-S (69M) \newline $\bullet$ MaxViT-B (120M) \newline $\bullet$ MaxViT-L (212M) \\

        Precisión (ImageNet-1K) & 
        $\bullet$ T: $\sim$82.1\% \newline $\bullet$ B: $\sim$83.8\% \newline $\bullet$ L: $\sim$84.3\% \newline $\bullet$ XL (21K): $\sim$87.8\% & 
        $\bullet$ T: $\sim$81.3\% \newline $\bullet$ B: $\sim$83.5\% \newline $\bullet$ L: $\sim$86.4\% \newline $\bullet$ L (21K): $\sim$87.3\% & 
        $\bullet$ T: $\sim$83.6\% \newline $\bullet$ B: $\sim$85.0\% \newline $\bullet$ L: $\sim$86.4\% \newline $\bullet$ L (21K): $\sim$89.5\% \\

        Velocidad de Entrenamiento & 
        Rápido (operaciones de convolución optimizadas) & 
        Moderado (overhead de atención) & 
        Moderado-lento (múltiples tipos de atención) \\

        Eficiencia de Inferencia & 
        Excelente (convoluciones muy optimizadas en GPU) & 
        Buena (atención local eficiente) & 
        Buena (pero más complejo que los otros) \\

        Backbone Universal & 
        \cmark\ Sí (detección, segmentación) & 
        \cmark\ Sí (muy popular para downstream tasks) & 
        \cmark\ Sí (clasificación, detección, segmentación) \\

        Mejor Para & 
        $\bullet$ Cuando se prefiere simplicidad de CNNs \newline $\bullet$ Inferencia eficiente \newline $\bullet$ Transfer learning & 
        $\bullet$ Backbone para tareas densas \newline $\bullet$ Estado del arte en detección/segmentación \newline $\bullet$ Preentrenamiento a gran escala & 
        $\bullet$ Máxima precisión \newline $\bullet$ Cuando se necesita contexto global desde el inicio \newline $\bullet$ Tareas que requieren relaciones de largo alcance \\

        Disponibilidad & 
        \cmark\ torchvision + \texttt{timm} & 
        \cmark\ torchvision + \texttt{timm} & 
        \cmark\ \texttt{timm} (\texttt{maxvit\_*}) \\

        Debilidad Principal & 
        Menor capacidad de modelar dependencias globales que Transformers & 
        Conexión entre ventanas limitada por shifted windows & 
        Mayor complejidad de implementación; menos maduro que Swin/ConvNeXt \\

        \bottomrule
    \end{tabularx}
    \caption{Comparación de Arquitecturas Modernas: ConvNeXt, Swin Transformer y MaxViT}
    \label{tab:modern_model_comparison}
\end{table*}

\subsubsection{Modelos clásicos}

En la tabla \ref{tab:model_comparison} se presentan las comparaciones detalladas de las 
diferentes arquitecturas clásicas estudiadas: GoogLeNet (Inception), ResNet, EfficientNetV1 y 
EfficientNetV2. Aquí se describen las características principales de cada modelo, 
sus ventajas y desventajas, y por qué se eligieron para este proyecto.

A lo largo del tiempo se han desarrollado técnicas interesantes que han llevado al avance de estos
modelos. A continuación los describiré brevemente:

\begin{enumerate}
    \item \textbf{GoogLeNet (Inception)}: Introdujo los módulos Inception que permiten capturar 
    características a múltiples escalas mediante convoluciones paralelas de diferentes tamaños. 
    Esto mejora la eficiencia computacional y la capacidad de extracción de características.
    \item \textbf{ResNEt (Capas Residuales)}: Introdujo las conexiones residuales (skip connections) 
    que facilitan el entrenamiento de redes muy profundas al mitigar el problema del desvanecimiento 
    del gradiente. Esto permitió construir redes con cientos de capas.
    \item \textbf{EfficientNetV1 (Escalado Compuesto)}: Propuso un método sistemático para escalar 
    redes neuronales ajustando conjuntamente la profundidad, anchura y resolución de entrada mediante 
    un coeficiente fijo. Esto llevó a modelos más eficientes en términos de precisión y tamaño.
    \item \textbf{EfficientNetv2 (Progesive Learning)}: Mejoró la velocidad de entrenamiento mediante
    una arquitectura más inteligente y una regularización adaptativa.  
    \item \textbf{MBConv (Mobile Inverted Bottleneck)}: Utilizado en EfficientNet, este bloque combina 
    convoluciones depthwise separables con una expansión y proyección de canales, lo que reduce 
    significativamente el número de parámetros y operaciones computacionales. Se basa en que las capas 
    más finas (bottlenecks) ya tienen la información necesaria, y las convoluciones depthwise son más eficientes.
    \item \textbf{Fused-MBConv}: Introducido en EfficientNetV2, este bloque combina convoluciones regulares
    con depthwise en las primeras etapas de la red para mejorar la velocidad de entrenamiento sin sacrificar 
    la precisión.
    \item \textbf{Squeeze-and-Excitation (SE)}: Introdujo bloques de atención que recalibran dinámicamente 
    las características de los canales, mejorando la capacidad del modelo para enfocarse en características 
    relevantes.
\end{enumerate}

Estas son algunas de las técnicas clave que han impulsado el desarrollo de modelos clásicos de clasificación
de imágenes. Cada una de ellas ha contribuido a mejorar la precisión, eficiencia y capacidad de generalización
de las redes neuronales convolucionales.

\subsubsection{Modelos modernos}

EL desarrollo de los Transformers ha revolucionado el campo del aprendizaje profundo, y su aplicación
a la visión por computadora ha llevado a la creación de modelos modernos que superan a las arquitecturas
clásicas en muchas tareas. En la tabla \ref{tab:modern_model_comparison} se presentan las comparaciones detalladas de las 
diferentes arquitecturas modernas estudiadas: ConvNeXt, Swin Transformer y MaxViT. Aquí se describen las características principales de cada modelo, 
sus ventajas y desventajas, y por qué se eligieron para este proyecto.

A continuación se describen brevemente las técnicas clave de estos modelos modernos:

\begin{enumerate}
\item \textbf{Convolución Depthwise Separable}: Fundamental en arquitecturas como ConvNeXt y EfficientNet (MBConv). Descompone una convolución estándar en dos operaciones más ligeras: una \textit{depthwise} (que opera espacialmente canal por canal de forma independiente) y una \textit{pointwise} (convolución 1$\times$1 que mezcla la información de los canales). Esto reduce drásticamente la cantidad de parámetros y FLOPs, permitiendo aumentar el tamaño de la red sin disparar el costo computacional.
    
\item \textbf{LayerNorm (Normalización de Capa)}: A diferencia de BatchNorm, que normaliza a través del lote (batch), LayerNorm normaliza las estadísticas a través de la dimensión de los canales para cada muestra individualmente. Originalmente popular en NLP, se ha convertido en el estándar para Vision Transformers y ConvNeXt debido a su estabilidad durante el entrenamiento y su independencia del tamaño del batch, lo cual es crucial cuando se trabaja con imágenes de alta resolución donde los batches son pequeños.

\item \textbf{GELU (Gaussian Error Linear Unit)}: Es una función de activación más suave que la tradicional ReLU. Mientras que ReLU trunca abruptamente a cero los valores negativos, GELU los pondera por su magnitud basándose en una distribución normal probabilística. Esta curvatura suave facilita la optimización y el flujo de gradientes en redes muy profundas, siendo la activación por defecto en BERT, GPT, ViT y ConvNeXt.

\item \textbf{Atención por Ventanas Desplazadas (Shifted Window Attention)}: Es la innovación clave del Swin Transformer. Calcular la auto-atención global es costoso ($O(N^2)$). Swin limita la atención a ventanas locales fijas (costo lineal $O(N)$), pero para que las ventanas se comuniquen entre sí, desplaza (shiftea) la partición de las ventanas en capas alternas. Esto crea conexiones cruzadas que permiten que la información se propague globalmente a través de la imagen con una eficiencia computacional muy alta.

\item \textbf{Atención Multi-Eje (Multi-Axis Attention)}: Utilizada en MaxViT para resolver la falta de contexto global eficiente. Descompone la atención dispersa en dos pasos: \textit{Block Attention} (local, que captura texturas y detalles cercanos) y \textit{Grid Attention} (global y diluida, que atiende píxeles lejanos en una rejilla regular). Juntos, permiten un campo receptivo global desde las primeras capas con una complejidad lineal, superando las limitaciones de las ventanas puramente locales.

\item \textbf{Sesgo de Posición Relativa (Relative Position Bias)}: Los Transformers, al ser invariantes a la permutación, necesitan información posicional. En lugar de sumar embeddings de posición absolutos a la entrada (como en el ViT original), modelos como Swin y MaxViT inyectan un sesgo aprendible directamente en la matriz de atención basado en la distancia relativa entre píxeles. Esto mejora la generalización a diferentes resoluciones de imagen, permitiendo entrenar en un tamaño y testear en otro diferente.

\item \textbf{Stem "Patchify" y Downsampling Separado}: Las CNN clásicas reducían la dimensión agresivamente al inicio. Los modelos modernos imitan a los Transformers dividiendo la imagen en "parches" no superpuestos (usando una convolución, por ejemplo, de 4$\times$4 con stride 4). Además, en lugar de usar \textit{max pooling} dentro de los bloques, utilizan capas de \textit{downsampling} dedicadas y separadas entre etapas (generalmente convoluciones con stride 2 y normalización) para estabilizar el entrenamiento profundo.

\end{enumerate}

Estos avances representan la convergencia entre los mundos de las CNNs y los Transformers: las CNNs modernas (ConvNeXt) adoptan la "macro-arquitectura" de los Transformers (GELU, LayerNorm, parches), mientras que los Transformers modernos (Swin, MaxViT) reintroducen la jerarquía y la localidad propias de las convoluciones.


\subsection{Selección de Modelos Propuesta (Revisada)}

Considerando la arquitectura del pipeline definido: \textit{Identificación de Hoja (Fase 1) $\rightarrow$ Extracción de Lesión vía YOLO (Fase 2) $\rightarrow$ Identificación de Enfermedad en recorte (Fase 3)}, la selección de modelos debe adaptarse a la naturaleza de los datos de entrada en cada etapa.

\subsubsection{Fase 1: Clasificación de Tipos de Planta}
\textbf{Entrada:} Imagen completa de la hoja/planta (con fondo, tallo, etc.).
\textbf{Modelo Elegido: EfficientNetV2 (Variante S)}

\textbf{Justificación:}
\begin{itemize}
\item \textbf{Reconocimiento de Formas:} Para distinguir entre tipos de plantas (ej. maíz vs. tomate), el modelo debe basarse en características geométricas de bajo y medio nivel (forma del borde, venación, estructura del tallo). Las redes convolucionales (CNNs) son naturalmente eficientes extrayendo estas características de forma.
\item \textbf{Velocidad de Entrada:} Al ser el primer filtro, EfficientNetV2 ofrece la mejor latencia. Su uso de \textit{Fused-MBConv} en las primeras capas procesa la imagen agresivamente sin perder información espacial relevante para la morfología de la planta.
\item \textbf{Robustez al Fondo:} EfficientNetV2, gracias a su entrenamiento con aumentos progresivos, suele generalizar bien incluso si el fondo de la imagen varía, centrándose en la estructura principal del objeto.
\end{itemize}

\subsubsection{Fase 3: Clasificación de Enfermedades (Sobre recortes)}
\textbf{Entrada:} Imagen recortada ("macro") centrada específicamente en la lesión o síntoma, extraída previamente por YOLO.
\textbf{Modelo Elegido: MaxViT (o ConvNeXt)}

\textbf{Justificación del cambio de enfoque:}
Al trabajar con recortes (crops), eliminamos el ruido del fondo y la necesidad de buscar la enfermedad. El problema se convierte en distinción de texturas de alta frecuencia. Por ejemplo, distinguir entre \textit{Tizón Temprano} (anillos concéntricos) y \textit{Mancha Bacteriana} (puntos negros con halo amarillo).

\textbf{Opción A: MaxViT (Recomendada por Precisión)}
\begin{itemize}
\item \textbf{Híbrido Perfecto para Macro:} MaxViT inicia cada bloque con capas \textit{MBConv} (convolucionales). Las convoluciones son excelentes detectores de texturas, bordes y gradientes de color, que son vitales en una fotografía "macro" de una enfermedad.
\item \textbf{Relación Centro-Periferia:} Aunque el recorte es pequeño, la atención (\textit{Block Attention}) permite al modelo relacionar el centro de la lesión (ej. tejido necrótico) con su periferia (ej. halo clorótico). Esta relación estructural es lo que define a muchas patologías, y MaxViT lo captura mejor que una CNN pura.
\end{itemize}

\textbf{Opción B: ConvNeXt (Recomendada por Coherencia/Simplicidad)}
\begin{itemize}
\item \textbf{Dominio de la Textura:} ConvNeXt es una arquitectura puramente convolucional pero modernizada. Históricamente, las CNNs superan a los Transformers puros (como ViT) en tareas donde la textura es más importante que la forma global. Al ser un recorte de la enfermedad, la "forma" importa menos que la "textura" del hongo o bacteria.
\item \textbf{Invarianza:} Al trabajar con recortes de YOLO, la lesión puede no estar perfectamente centrada o tener distintos tamaños. ConvNeXt maneja muy bien estas variaciones sin necesidad de embeddings posicionales complejos.
\end{itemize}

\subsubsection{Conclusión de la Selección}

Se propone utilizar \textbf{EfficientNetV2} para la Fase 1 debido a su eficiencia en características morfológicas globales, y \textbf{MaxViT} para la Fase 3.

La elección de \textbf{MaxViT} sobre un Transformer puro (como Swin) para la Fase 3 es estratégica: al tener componentes convolucionales fuertes (MBConv), no pierde la capacidad de analizar la "rugosidad" y los detalles finos de la enfermedad, pero su mecanismo de atención le permite entender la estructura compleja de la lesión mejor que una CNN clásica. Es el modelo que mejor se adapta a la variabilidad visual de las enfermedades en imágenes de primer plano.